{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download white matter steamlines from the Allen Brain Institute\n",
    "![](../bioexplorer_white_matter_banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61708edf",
   "metadata": {},
   "source": [
    "## Pre-requeries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8db0dd",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install allensdk\n",
    "pip install transformations\n",
    "pip install sklearn\n",
    "\n",
    "git clone https://github.com/AllenInstitute/mouse_connectivity_models.git\n",
    "cd mouse_connectivity_models\n",
    "pip install .\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce5a6ff",
   "metadata": {},
   "source": [
    "## StreamlineDownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mcmodels\n",
    "import numpy\n",
    "\n",
    "q_str = \"http://connectivity.brain-map.org/projection/csv?criteria=service::mouse_connectivity_target_spatial[injection_structures$eq%d][seed_point$eq%d,%d,%d][primary_structure_only$eq%s]\"\n",
    "HEMISPHERE_RIGHT=\"right\"\n",
    "HEMISPHERE_LEFT=\"left\"\n",
    "\n",
    "class StreamlineDownloader(object):\n",
    "    \"\"\"\n",
    "    Original source code from Michael Reimann: michael.reimann@epfl.ch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, manifest_file=None, tmp_dir=None):\n",
    "        self.__set_cache__(manifest_file)\n",
    "        self.__set_tmp_dir__(tmp_dir)\n",
    "\n",
    "    def __set_cache__(self, manifest_file):\n",
    "        if manifest_file is None:\n",
    "            manifest_file = os.path.join(os.getenv(\"HOME\", '.'), 'data/connectivity/voxel_model_manifest.json')\n",
    "        if not os.path.exists(os.path.split(manifest_file)[0]):\n",
    "            os.makedirs(os.path.split(manifest_file)[0])\n",
    "        self._cache = mcmodels.core.VoxelModelCache(manifest_file=manifest_file)\n",
    "        self._tree = self._cache.get_structure_tree()\n",
    "        self._vol, self._vol_spec = self._cache.get_annotation_volume()\n",
    "        self._lr_cutoff = self._vol.shape[-1] / 2\n",
    "\n",
    "    def __set_tmp_dir__(self, tmp_dir):\n",
    "        if tmp_dir is None:\n",
    "            tmp_dir = os.path.join(os.getenv(\"HOME\", '.'), 'data/connectivity/sl_cache')\n",
    "        if not os.path.exists(tmp_dir):\n",
    "            os.makedirs(tmp_dir)\n",
    "        self._sl_cache = tmp_dir\n",
    "        \n",
    "    def __region2center__(self, region_acronym, hemisphere='right'):\n",
    "        idxx = self._tree.get_structures_by_acronym([region_acronym])[0]['id']\n",
    "        mask = numpy.in1d(self._vol, self._tree.descendant_ids([idxx])[0]).reshape(self._vol.shape)\n",
    "        lr_cutoff = int(self._lr_cutoff)\n",
    "        if hemisphere == 'right':\n",
    "            mask[:, :, :lr_cutoff] = False\n",
    "        elif hemisphere == 'left':\n",
    "            mask[:, :, lr_cutoff:] = False\n",
    "        else:\n",
    "            raise ReferenceError(\"Unknown hemisphere: %s\" % hemisphere)\n",
    "        center = numpy.vstack(numpy.nonzero(mask)).mean(axis=1)\n",
    "        ret = center * numpy.matrix(self._vol_spec['space directions']).astype(float)\n",
    "        return numpy.array(ret)[0]\n",
    "\n",
    "    def __execute_query__(self, q, suffix):\n",
    "        import requests\n",
    "        from os.path import exists\n",
    "        \n",
    "        fn = \"tmp_sl_%s.csv\" % suffix\n",
    "        fn = os.path.join(self._sl_cache, fn)\n",
    "        \n",
    "        if not exists(fn):\n",
    "            response = requests.get(q)\n",
    "            with open(fn, 'w') as f:        \n",
    "                f.write(response.text)\n",
    "                f.close()\n",
    "        return fn\n",
    "\n",
    "    def __coords2hemi__(self, pt):\n",
    "        M = numpy.matrix(self._vol_spec['space directions']).astype(int)\n",
    "        raw_pt = numpy.diag(pt / M)\n",
    "        if raw_pt[2] >= self._lr_cutoff:\n",
    "            return 'right'\n",
    "        return 'left'\n",
    "\n",
    "    def get_tree(self):\n",
    "        return self._tree\n",
    "        \n",
    "    def add_hemisphere_info(self, a_result):\n",
    "        \"\"\"Look up for a dict of streamlines which hemisphere they originate in.\n",
    "        Returns a dict where the keys are a tuple of the original keys and a string\n",
    "        specifying the hemisphere\"\"\"\n",
    "        out = {}\n",
    "        for k, v in a_result.items():\n",
    "            for sl in v:\n",
    "                out.setdefault((k, self.__coords2hemi__(sl[-1])), []).append(sl)\n",
    "        return out\n",
    "\n",
    "    def get_query_string(self, target_spec, target_hemisphere='right', source_spec='grey', primary_only=False):\n",
    "        \"\"\"Build a url to query the Allen servers for a set of streamlines from / to specified regions.\n",
    "        INPUT:\n",
    "        target_spec: Specify the approximate endpoint of streamlines. Either a string naming a brain region (VISp, etc.)\n",
    "        or a list of three coordinates. If a string is specified the center of that region in the specified hemisphere is used.\n",
    "        target_hemisphere: Only if target_spec is a string.\n",
    "        source_spec: Either a string naming a brain region (SSp-ll, etc.) or a brain region id.\n",
    "        primary_only: If true then the primary injection structure must be according to source spec.\n",
    "\n",
    "        RETURNS:\n",
    "            a query string \"\"\"\n",
    "        if isinstance(source_spec, str):\n",
    "            source_spec = self._tree.get_structures_by_acronym([source_spec])[0]['id']\n",
    "        if isinstance(target_spec, str):\n",
    "            target_spec = self.__region2center__(target_spec, hemisphere=target_hemisphere).astype(int)\n",
    "        return q_str % ((source_spec, ) + tuple(target_spec) + (str(primary_only).lower(), ))\n",
    "\n",
    "    def query(self, target_spec, target_hemisphere='right', source_spec='grey',\n",
    "              primary_only=True, add_hemispheres=True):\n",
    "        \"\"\"Query Allen servers for streamlines from / to specified regions.\n",
    "        INPUT:\n",
    "        target_spec: Specify the approximate endpoint of streamlines. Either a string naming a brain region (VISp, etc.)\n",
    "        or a list of three coordinates. If a string is specified the center of that region in the specified hemisphere is used.\n",
    "        target_hemisphere: Only if target_spec is a string.\n",
    "        source_spec: Either a string naming a brain region (SSp-ll, etc.) or a brain region id.\n",
    "        primary_only: If true then the primary injection structure must be according to source spec.\n",
    "        add_hemispheres: If true then separate sets of streamlines are returned for each hemisphere.\n",
    "\n",
    "        RETURNS:\n",
    "            A dict where keys are source regions (or a tuple of source region and source hemisphere) and values the\n",
    "            streamlines (lists of Nx3 numpy.arrays)\n",
    "        \"\"\"\n",
    "        q = self.get_query_string(target_spec, target_hemisphere=target_hemisphere,\n",
    "                                  source_spec=source_spec, primary_only=primary_only)\n",
    "        suffix = '%s_%s_%s' % (target_hemisphere, target_spec.replace('/', '-'), source_spec.replace('/', '-'))\n",
    "        fn = self.__execute_query__(q, suffix)\n",
    "        sls = self.import_streamlines_from_csv(fn)\n",
    "        if add_hemispheres:\n",
    "            return self.add_hemisphere_info(sls)\n",
    "        return sls\n",
    "\n",
    "    def query_lengths(self, target_spec, target_hemisphere='right', source_spec='grey',\n",
    "                      primary_only=True, add_hemispheres=True):\n",
    "        \"\"\"Look up lengths of streamlines from / to specified regions.\n",
    "        INPUT:\n",
    "        target_spec: Specify the approximate endpoint of streamlines. Either a string naming a brain region (VISp, etc.)\n",
    "        or a list of three coordinates. If a string is specified the center of that region in the specified hemisphere is used.\n",
    "        target_hemisphere: Only if target_spec is a string.\n",
    "        source_spec: Either a string naming a brain region (SSp-ll, etc.) or a brain region id.\n",
    "        primary_only: If true then the primary injection structure must be according to source spec.\n",
    "        add_hemispheres: If true then separate sets of streamline lengths are returned for each hemisphere.\n",
    "\n",
    "        RETURNS:\n",
    "            A dict where keys are source regions (or a tuple of source region and source hemisphere) and values the\n",
    "            lengths of streamlines (lists floats)\n",
    "                \"\"\"\n",
    "        res = self.query(target_spec, target_hemisphere=target_hemisphere, source_spec=source_spec,\n",
    "                         primary_only=primary_only, add_hemispheres=add_hemispheres)\n",
    "        return dict([(k, map(self.streamline_length, v)) for k, v in res.items()])\n",
    "\n",
    "    @staticmethod\n",
    "    def import_streamlines_from_csv(filename):\n",
    "        import csv\n",
    "        import json\n",
    "        streams = {}\n",
    "        with open(filename, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter=',')\n",
    "            next(reader, None)  # skip the headers\n",
    "            for row in reader:\n",
    "                stream = []\n",
    "                streamline = row[16].replace('=>', ':')\n",
    "                source_region = row[4]\n",
    "                j = json.loads(streamline)\n",
    "                for c in j:\n",
    "                    point = c['coord']\n",
    "                    stream.append([(float)(point[0]), (float)(point[1]), (float)(point[2])])\n",
    "                streams.setdefault(source_region, []).append(numpy.array(stream))\n",
    "            csvfile.close()\n",
    "        return streams\n",
    "\n",
    "    @staticmethod\n",
    "    def streamline_length(a_line):\n",
    "        return numpy.sqrt(numpy.sum(numpy.diff(a_line, axis=0) ** 2, axis=1)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sld = StreamlineDownloader()\n",
    "tree = sld.get_tree()\n",
    "structures = tree.get_id_acronym_map()\n",
    "names = tree.get_name_map()\n",
    "del structures['root']\n",
    "del structures['grey']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c1dbbb",
   "metadata": {},
   "source": [
    "## Connect to PostgreSQL DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection string: postgresql+psycopg2://bioexplorer:verole2020@postgresql14-users.bbp.epfl.ch:5432/bioexplorer, schema: connectome\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import Session\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "db_host = os.getenv('DB_HOST')\n",
    "db_name = os.getenv('DB_NAME')\n",
    "db_user = os.getenv('DB_USER')\n",
    "db_password = os.getenv('DB_PASSWORD')\n",
    "db_schema = 'connectome'\n",
    "\n",
    "db_connection_string = 'postgresql+psycopg2://%s:%s@%s:5432/%s' % (db_user, db_password, db_host, db_name)\n",
    "print('Connection string: ' + db_connection_string + ', schema: ' + db_schema)\n",
    "\n",
    "engine = create_engine(db_connection_string)\n",
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa99894",
   "metadata": {},
   "source": [
    "### Import regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with Session(engine) as session:\n",
    "    for key in structures.keys():\n",
    "        session.execute(\n",
    "            'INSERT INTO %s.structure VALUES (:guid, :code)' % db_schema,\n",
    "            {\n",
    "                'guid': structures[key],\n",
    "                'code': key\n",
    "            }\n",
    "        )\n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d424dd9e",
   "metadata": {},
   "source": [
    "### Import streamlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "hemispheres = dict()\n",
    "hemispheres['right'] = 0\n",
    "hemispheres['left'] = 1\n",
    "\n",
    "def load_streamlines_into_db(source, target):\n",
    "    data = sld.query(\n",
    "        source_spec=source,\n",
    "        target_spec=target,\n",
    "        primary_only=False)\n",
    "    with Session(engine) as session:\n",
    "        guid = 0\n",
    "        for target_region in data:\n",
    "            hemisphere = hemispheres[target_region[1]]\n",
    "            for streamline in data[target_region]:\n",
    "                points = list()\n",
    "                for point in streamline:\n",
    "                    for value in point:\n",
    "                        points.append(value)\n",
    "                buffer = np.array(points, dtype=np.float32).tobytes()\n",
    "                session.execute(\n",
    "                    'INSERT INTO %s.streamline VALUES (:guid, :hemisphere, :origin, :target, :buffer)' % db_schema,\n",
    "                    {\n",
    "                        'guid': guid,\n",
    "                        'hemisphere': hemisphere,\n",
    "                        'origin': structures[source],\n",
    "                        'target': structures[target],\n",
    "                        'buffer': buffer\n",
    "                    }\n",
    "                )\n",
    "                guid += 1\n",
    "        session.commit()\n",
    "\n",
    "with open('import.log', 'w') as f:\n",
    "    for hemisphere in range(2):\n",
    "        for origin in tqdm(structures.keys()):\n",
    "            for target in tqdm(structures.keys()):\n",
    "                if origin != target:\n",
    "                    try:\n",
    "                        load_streamlines_into_db(origin, target)\n",
    "                    except Exception as e:\n",
    "                        f.write('%s_%s_%s: %s' % (hemisphere, origin, target, e))\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ea9a5fa46eb6bad2806a8ea1d08e15bb1e255a2d4320b81e765591579963c56b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
